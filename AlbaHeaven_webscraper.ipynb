{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "\r\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xh\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "alba_result = requests.get(\"http://www.alba.co.kr/\")\n",
    "#let's check if requests works fine\n",
    "print(alba_result.status_code)\n",
    "print(alba_result.text[:100])\n",
    "\n",
    "alba_soup = BeautifulSoup(alba_result.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get links to \"슈퍼브랜드\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of \"super brands\": 128\n",
      "\n",
      "이자녹스/비욘드/네이처컬렉션 http://lghnh.alba.co.kr/\n",
      "딜리온 http://delion.alba.co.kr/\n",
      "SSG.COM http://ssg.alba.co.kr/\n"
     ]
    }
   ],
   "source": [
    "#get links to \"super brands\" on the main page of www.alba.co.kr, save save them on links\n",
    "super_brand = alba_soup.find(\"div\", {\"id\":\"MainSuperBrand\"})\n",
    "super_brand_box = super_brand.find(\"ul\",{\"class\":\"goodsBox\"})\n",
    "lis = super_brand_box.find_all(\"li\")\n",
    "\n",
    "links = []\n",
    "for li in lis:\n",
    "    for a in li.find_all('a',{\"class\":\"goodsBox-info\"}):\n",
    "        company_name = a.find(\"span\",{\"class\":\"company\"}).text\n",
    "        url = a['href']\n",
    "        links.append({'company_name':company_name, 'url':url})\n",
    "    \n",
    "print(f\"number of \\\"super brands\\\": {len(links)}\\n\")\n",
    "#test\n",
    "for link in links[:3]:\n",
    "    print(link.get('company_name'), link.get('url'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape jobs and save them in csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scraping 이자녹스/비욘드/네이처컬렉션 http://lghnh.alba.co.kr/...\n",
      "last page : 1\n",
      "extracting http://lghnh.alba.co.kr/job/brand/?page=1...\n",
      "\n",
      "scraping 딜리온 http://delion.alba.co.kr/...\n",
      "last page : 3\n",
      "extracting http://delion.alba.co.kr/job/brand/?page=1...\n",
      "extracting http://delion.alba.co.kr/job/brand/?page=2...\n",
      "extracting http://delion.alba.co.kr/job/brand/?page=3...\n",
      "\n",
      "scraping SSG.COM http://ssg.alba.co.kr/...\n",
      "last page : 2\n",
      "extracting http://ssg.alba.co.kr/job/brand/?page=1...\n",
      "extracting http://ssg.alba.co.kr/job/brand/?page=2...\n",
      "\n",
      "scraping 교촌치킨 http://kyochon.alba.co.kr/...\n",
      "last page : 19\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=1...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=2...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=3...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=4...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=5...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=6...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=7...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=8...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=9...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=10...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=11...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=12...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=13...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=14...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=15...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=16...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=17...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=18...\n",
      "extracting http://kyochon.alba.co.kr/job/brand/?page=19...\n",
      "\n",
      "scraping 7번가피자 http://7thpizza.alba.co.kr/...\n",
      "last page : 2\n",
      "extracting http://7thpizza.alba.co.kr/job/brand/?page=1...\n",
      "extracting http://7thpizza.alba.co.kr/job/brand/?page=2...\n",
      "\n",
      "scraping 러쉬코리아 http://lushkorea.alba.co.kr/...\n",
      "last page : 1\n",
      "extracting http://lushkorea.alba.co.kr/job/brand/?page=1...\n",
      "\n",
      "scraping 맥도날드 http://mcdonalds.alba.co.kr/...\n",
      "last page : 16\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=1...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=2...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=3...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=4...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=5...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=6...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=7...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=8...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=9...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=10...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=11...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=12...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=13...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=14...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=15...\n",
      "extracting http://mcdonalds.alba.co.kr/job/brand/?page=16...\n",
      "\n",
      "scraping 청소연구소 http://cleaninglab.alba.co.kr/...\n",
      "last page : 18\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=1...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=2...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=3...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=4...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=5...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=6...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=7...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=8...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=9...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=10...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=11...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=12...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=13...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=14...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=15...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=16...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=17...\n",
      "extracting http://cleaninglab.alba.co.kr/job/brand/?page=18...\n",
      "\n",
      "scraping 호식이두마리치킨 http://hosigi.alba.co.kr/...\n",
      "last page : 2\n",
      "extracting http://hosigi.alba.co.kr/job/brand/?page=1...\n",
      "extracting http://hosigi.alba.co.kr/job/brand/?page=2...\n",
      "\n",
      "scraping 피자샵 http://handmadepizza.alba.co.kr/...\n",
      "last page : 6\n",
      "extracting http://handmadepizza.alba.co.kr/job/brand/?page=1...\n",
      "extracting http://handmadepizza.alba.co.kr/job/brand/?page=2...\n",
      "extracting http://handmadepizza.alba.co.kr/job/brand/?page=3...\n",
      "extracting http://handmadepizza.alba.co.kr/job/brand/?page=4...\n",
      "extracting http://handmadepizza.alba.co.kr/job/brand/?page=5...\n",
      "extracting http://handmadepizza.alba.co.kr/job/brand/?page=6...\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import csv\n",
    "\n",
    "def get_last_page(soup):\n",
    "    normal_info = soup.find(\"div\",{\"id\":\"NormalInfo\"})\n",
    "    job_count = normal_info.find(\"p\", {\"class\":\"jobCount\"}).find(\"strong\").string\n",
    "    page = int(job_count.replace(',',''))/50.0\n",
    "    print(f\"last page : {math.ceil(page)}\")\n",
    "    return math.ceil(page)\n",
    "\n",
    "def extract_job_from_a_page(URL):\n",
    "    print(f\"extracting {URL}...\")\n",
    "    result = requests.get(URL)\n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    trs = soup.find_all(\"tr\") #one <tr> contains one job\n",
    "    jobs = []\n",
    "    for tr in trs:\n",
    "        if(tr.find(\"td\",{\"class\":\"local first\"}) is None):\n",
    "            continue\n",
    "        place = tr.find(\"td\",{\"class\":\"local first\"}).text.strip()\n",
    "        title = tr.find(\"span\",{\"class\":\"title\"}).text.strip()\n",
    "        working_hours = tr.find(\"td\",{\"class\":\"data\"}).text.strip()\n",
    "        remuneration = tr.find(\"td\",{\"class\":\"pay\"}).text.strip()\n",
    "        last_update = tr.find(\"td\",{\"class\":\"regDate last\"}).text.strip()\n",
    "        jobs.append({'place':place,\n",
    "                    'title':title,\n",
    "                    'working_hours':working_hours,\n",
    "                    'remuneration':remuneration,\n",
    "                    'last_update':last_update})\n",
    "        \n",
    "    return jobs\n",
    "\n",
    "def save_to_file(company_name, jobs):\n",
    "    company_name = company_name.replace('/','_')\n",
    "    with open(f\"{company_name}.csv\", 'w', newline=\"\") as csv_file:\n",
    "        columns = [\"place\",\"title\",\"working_hours\",\"remuneration\",\"last_update\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames = columns)\n",
    "        writer.writeheader()\n",
    "        for job in jobs:\n",
    "            writer.writerow(job)\n",
    "\n",
    "def extract_from_one_brand(company_name, URL):\n",
    "    result = requests.get(URL)\n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    last_page = get_last_page(soup)\n",
    "    all_jobs = []\n",
    "    \n",
    "    for i in range(last_page):\n",
    "        jobs = extract_job_from_a_page(f\"{URL}job/brand/?page={i+1}\")\n",
    "        all_jobs = all_jobs + jobs\n",
    "    \n",
    "    save_to_file(company_name, all_jobs)\n",
    "\n",
    "\n",
    "#Here is where the code starts\n",
    "#limit the number of files to 10 (I don't want to save more than 100 files)\n",
    "limit=10\n",
    "for link in links[:limit]:\n",
    "    print(f\"\\nscraping {link.get('company_name')} {link.get('url')}...\")\n",
    "    extract_from_one_brand(link.get('company_name'), link.get('url'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
